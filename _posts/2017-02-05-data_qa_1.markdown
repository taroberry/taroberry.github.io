---
layout: post
title:  大数据平台数据质量(一) —— 问题描述
category: bigdata
description: 数据质量是数据领域里很重要的一个话题，大部分关于数据仓库、大数据平台的文章都会提到数据质量，但是往往是一带而过，计划接下来写一系列的文章，详细介绍有赞数据平台在数据质量方面的实践。本文描述了遇到的数据质量的问题和需求。
---
数据质量是数据领域里很重要的一个话题，大部分关于数据仓库、大数据平台的文章都会提到数据质量，但是往往是一带而过，计划接下来写一系列的文章，详细介绍有赞数据平台在数据质量方面的实践。

数据质量，换句话说，就是数据不准确、有问题。一个数据平台或者数据应用，数据准确性是最重要的。如果数据不准，性能再好、查询再灵活、数据实时性再好，也是白搭。

一个简化的、典型的互联网公司数据仓库是这样的，业务数据和服务器日志，通过各种实时/离线的ETL方案，存储到hadoop中，并在hadoop里构建数据仓库。有个存储原始数据的ODS库，建模后形成DW库，最终在DW基础上做各种数据应用。

<img src="{{site.baseurl}}/assets/img/数据质量-数据仓库简单架构.png" width="95%" />

各个环节可能都会数据质量问题，下面举了些例子：

* 关系型业务数据：业务本身存在bug或不一致，比如订单状态是已付款，但是没有付款记录
* 日志采集：
  * 信息没采集：用户首次访问网站时，服务器日志的cookie部分是空的；按钮没加埋点代码，导致用户点击行为采集丢失
  * 信息传递丢失：在hybrid app里，从native页面跳转带h5页面时，http_referer默认是空的；业务代码里改写了url导致参数传丢
  * 日志配置不合理：以秒为精度记录日志的话，用户一秒操作多次，就难以判断先后顺序；服务器默认记录的单条日志长度是有限的，有些很长的日志就会被截断，比如百度跳转过来的url就很长很长
* ETL：
  * ETL系统的问题：ETL工具挂了；队列重传乱序、网络丢包引起的数据丢失或错误
  * ETL配置和源数据不一致：连错ip端口，比如连到测试库了，又比如数据库地址换了
  * ETL配置和目标数据不一致：写错了hdfs目录；hdfs是orc格式，但ETL工具配置成了textfile
  *  源和目标schema不一致：源表的schema发生变化，但是ODS的没改；日志格式变了，但是解析程序没改
* 数据仓库建模：
  * 异常数据：用户刷单订单量虚高；测试人员创建的测试店铺、商品
  * 时效性问题：有些表已经很久没去更新数据了
  * bug：模型会不断迭代，开发的过程中难免出现bug
* 数据应用：
  * 口径不一致：报表A和B都有显示商品销量，A显示的是10，B显示的却是20
  * bug：数据应用也是不断开发，也难免出现bug

不难看出，大部分的问题都出在代码bug和各种不一致上，并且数据源头上的问题会很多。实际应用中，源头的错误影响也会更大，比如ETL的一个错误可能会导致一天所有的任务错误。问题的解决在后续文章描述。

